---
title: "Statistical Modeling: The Two Cultures Leo Breiman"
subtitle: "Amelia Chowaniec"
format:
    revealjs:
        slide-number: true
        preview-links: true
        theme: default
---
<style>
  /* Custom CSS to make the title font smaller */
  .reveal h1 {
      font-size: 60px;  /* Adjust the size as needed */
  }
  .small-break {
      line-height: 0.5; /* Adjust the value as needed */
  }
  .small-text {
      font-size: 0.7em; /* Use this class for smaller text */
  }
</style>

### The Two Cultures
<br>
Data Modeling Culture
<div class="small-text">
- Assumes that the data is generated by a given stochastic (random) data model. Data modeling aims to explain rather than simply predict.
</div>
<br>
Algorithmic Model
<div class="small-text">
- Treats the data mechanism as unknown. Algorithmic is a more flexible approach where prediction is the main goal.
</div>
<br>
Breiman's Argument
<div class="small-text">
- Breiman argues that algorithmic learning is superior to data modeling as it can be used on both larger complex data sets and as a more accurate and informative alternative on smaller data sets.
</div>

---

### Goals in Data Analysis
There are two goals when analyzing data:

<br>

<p align="center">
    <img src="nature.png" alt="Nature" width="600">
</p>
<span style="font-size: 30px;">Prediction: To be able to predict what the responses are going to be to future outputs</span>

<br>

<span style="font-size: 30px;">Information: To extract information about how nature is associating the response variables to the input variables</span>

---

### Data Modeling
In data modeling, the response variable can be expressed as:

<br>

<span style="font-size: 30px;">
$$
\text{response variable} = f(\text{predictor variables}, \text{random noise}, \text{other variables})
$$
</span>

<p align="center">
    <img src="data modeling.png" alt="Data Modeling" width="600">
</p>
<span style="font-size: 30px;">Model Validation: Yes-No using goodness of fit tests and residual examination</span>

<br>

<span style="font-size: 30px;">Estimated Culture Population: 98% of all Statisticians</span>

---

### Algorithmic Modeling
<span style="font-size: 30px;">Algorithmic modeling considers the box complex and unknown</span>

<br>

<p align="center">
    <img src="algorithmic.png" alt="Algorithmic Model" width="500">
</p>
<span style="font-size: 30px;">Model Validation: Measured by predictive accuracy, predictive accuracy is the central measure of success.</span>

<br>

<span style="font-size: 30px;">Estimated Culture Population: 2% of statisticians, many in other fields</span>

---

### The Use of Data Models

<br>

<div class="small-text">
- Statisticians rely on data modeling to guide their statistical analysis, assuming they can create models that represent complex natural processes.

<br>

- The conclusions drawn from these models refer to the models themselves, not directly to nature, which can result in errors if the models don't accurately reflect reality.

<br>

- So much faith is put in data modeling that important checks like residual analysis and goodness-of-fit tests can be overlooked, treating the outcomes as indisputable truths.
</div>

---

### The Problems with Data Modeling

<br>

<div class="small-text">
- Goodness of fit tests have very little power and fail to detect poor fit until it is extreme.

<br>

- An acceptable residual plot does not imply that the model is a good fit to the data

<br>

- Misleading conclusions may follow from data models that pass goodness of fit tests and residual checks, even "good" fits may not capture data complexity well.

</div>

---

### The Multiplicity of Data Models

<br>

The greatest plus of data modeling is that it produces a simple and understandable picture of the relationship between input variables and responses

<br>

Different models, all equally good, may give different pictures of the relation between the predictor and response variables

- One reason for this multiplicity is that goodness of fit tests give a yes-no answer


---

### The Limitations of Data Models

<br>

- Commonly, simple parametric models imposed on data generated by complex systems results in a loss of accuracy and information as compared to algorithmic models

<br>

- As data becomes more complex, the data models become more unmanageable and lose the advantage of presenting a simple, clear picture of nature's mechanism

---

### Algorithmic Modeling and Predictive Accuracy
<br>
There are three lessons that Breiman considers most important to predictive accuracy:

<br>
<div class="small-text">
1. Rashomon and the Multiplicity of Good Models  
   - There is no single “best” model; several models can yield equally accurate predictions.

2. Occam and Simplicity vs. Accuracy  
   - Sometimes complex models are necessary for accuracy

3. Bellman and the Curse of Dimensionality  
   - While high-dimensional data can pose problems, algorithmic models often manage this complexity better.

</div>
---

### Rashomon and the Multiplicity of Good Models

<br>

The Rashomon Effect suggests that many different function descriptions can yield similar error rates

- In a linear regression scenario with 30 variables, there are about 140,000 possible combinations of five variables.

- The common approach is to choose the combination with the lowest residual sum-of-squares (RSS). However, multiple near-optimal combinations can exist.

---

### Occam and the Simplicity vs. Accuracy
<div class="small-text">
<br>

- Occam’s Razor suggests that simpler models are preferable, but in practice, there is often a trade-off between simplicity and accuracy in predictions.

<br>

- While decision trees offer high interpretability, they may lack predictive accuracy compared to more complex models like neural networks or random forests.

<br>

- Random forests improve predictive accuracy significantly by using multiple trees for classification, but their complexity makes them difficult to interpret, leading to the "Occam dilemma."

</div>
---

### Bellman and the Curse of Dimensionality

<br>
<div class="small-text">
- The "curse of dimensionality" suggests too many predictor variables can hurt predictions, but recent studies show that more dimensions can actually improve accuracy.

<br>

- Methods like Shape Recognition Forest and Support Vector Machines (SVMs) show that adding features can significantly boost classification accuracy, such as in recognizing handwritten numbers.

<br>

- SVMs find the best decision boundaries for classification, and while more dimensions often help separate data, too much complexity can lead to errors, so finding a balance is important.
</div>
---

### Information from a Black Box
<br>

Logistic Regression
<div class="small-text">
- In logistic regression, the importance of variables is assessed by the absolute values of their standardized coefficients, but using only the most significant variables can increase the error rate.
<br>
- The best subsets search for identifying important variables presents challenges related to instability and model multiplicity.
</div>

Random Forests
<div class="small-text">
- Random forests significantly reduce predictive error compared to logistic regression by using many randomly constructed trees.
- This analysis finds the combination of variables that minimizes the error (deviance). This method can lead to issues with instability and many different models.
</div>

---

### Comments - Agreement Points

<br>

Importance of Applications: 
<div class="small-text">
- Both Professor Breiman and the commentators agree on the vast range of applications in statistics and the need to distinguish between different types of analyses based on context, highlighting that a generalized approach may overlook critical nuances.
</div>

<br>

Value of Non-Probabilistic Work: 
<div class="small-text">
- There is a shared understanding that ignoring valuable work just because it doesn’t have a clear probabilistic basis is a limited way of looking at the field.
</div>

---

### Comments - Disagreement Points

<br>

Starting Point of Analysis: 
<div class="small-text">
- While Breiman emphasizes starting with data, the commentators prefer beginning with a specific issue or hypothesis, suggesting that data-driven approaches may lead to atypical or less insightful analyses.
</div>

<br>

Role of Underlying Processes: 
<div class="small-text">
- Breiman adopts a black-box approach focusing on prediction, whereas the commentators stress the importance of understanding underlying processes, arguing that understanding the "why" behind predictions is just as important as the "what" in terms of accuracy.
- This describes a debate in the field about whether the primary goal should be accurate predictions or a deeper understanding of the processes that generate the data.
</div>

---

### Breiman's Response
<br>
<div class="small-text">

- Breiman disagrees with D. R. Cox's argument for traditional data models by emphasizing a more pragmatic approach to statistical problems that incorporates diverse methods, including algorithmic models.

<br>

- While acknowledging the usefulness of data models in certain contexts, Breiman argues that relying solely on them can be limiting, especially given the complexity and rapid evolution of statistical problems in fields like genetics and astronomy.

<br>

- Breiman stresses the need for statisticians to be opportunistic and adaptable, using whatever methods are most effective for the problem, including algorithmic approaches that may not provide easy interpretability but excel in predictive accuracy.
</div>

---

### Conclusion
<div class="small-text">

Data Models vs. Algorithmic Models

  - While data models aim for interpretability, they often fall short in accuracy, especially with complex data.

<br>

Importance of Predictive Accuracy

  - Breiman advocates for algorithmic models due to their superior predictive performance.

<br>

Future Directions

  - As data complexity grows, embracing a flexible approach that combines both modeling strategies will enhance our ability to extract meaningful insights and make accurate predictions. Statisticians must be adaptable and open to diverse methods to tackle evolving challenges in data analysis.

</div>